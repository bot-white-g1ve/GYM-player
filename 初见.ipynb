{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_type(default):\n",
    "  def parse_string(x):\n",
    "    if default is None:\n",
    "      return x\n",
    "    if isinstance(default, bool):\n",
    "      return bool(['False', 'True'].index(x))\n",
    "    if isinstance(default, int):\n",
    "      return float(x) if ('e' in x or '.' in x) else int(x)\n",
    "    if isinstance(default, (list, tuple)):\n",
    "      return tuple(args_type(default[0])(y) for y in x.split(','))\n",
    "    return type(default)(x)\n",
    "  def parse_object(x):\n",
    "    if isinstance(default, (list, tuple)):\n",
    "      return tuple(x)\n",
    "    return x\n",
    "  return lambda x: parse_string(x) if isinstance(x, str) else parse_object(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# os.environ['MUJOCO_GL'] = 'egl' # if no display\n",
    "os.environ['MUJOCO_GL'] = 'glfw'\n",
    "\n",
    "import numpy as np\n",
    "import ruamel.yaml as yaml\n",
    "\n",
    "import exploration as expl\n",
    "import models\n",
    "import tools\n",
    "import wrappers\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions as torchd\n",
    "to_np = lambda x: x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = argparse.Namespace(act='ELU', action_repeat=2, actor_disc=5, actor_dist='trunc_normal', actor_entropy='1e-4', actor_grad_clip=100, actor_init_std=1.0, actor_layers=4, actor_lr=8e-05, actor_min_std=0.1, actor_outscale=0.0, actor_state_entropy=0.0, actor_temp=0.1, batch_length=50, batch_size=50, behavior_stop_grad=True, clip_rewards='identity', cnn_depth=32, collect_dyn_sample=True, dataset_size=0, debug=False, decoder_kernels=(5, 5, 6, 6), decoder_thin=True, device='cuda:0', disag_action_cond=False, disag_layers=4, disag_log=True, disag_models=10, disag_offset=1, disag_target='stoch', disag_units=400, discount=0.99, discount_lambda=0.95, discount_layers=3, discount_scale=1.0, dyn_cell='gru', dyn_deter=200, dyn_discrete=0, dyn_hidden=200, dyn_input_layers=1, dyn_mean_act='none', dyn_min_std=0.1, dyn_output_layers=1, dyn_rec_depth=1, dyn_shared=False, dyn_std_act='sigmoid2', dyn_stoch=50, dyn_temp_post=True, encoder_kernels=(4, 4, 4, 4), envs=1, eval_every=10000.0, eval_noise=0.0, eval_state_mean=False, evaldir=None, expl_amount=0.0, expl_behavior='greedy', expl_extr_scale=0.0, expl_gifs=False, expl_intr_scale=1.0, expl_until=0, future_entropy=False, grad_clip=100, grad_heads=('image', 'reward'), grayscale=False, imag_gradient='dynamics', imag_gradient_mix='0.1', imag_horizon=15, imag_sample=True, kl_balance='0.8', kl_forward=False, kl_free='1.0', kl_scale='1.0', log_every=10000.0, logdir=None, model_lr=0.0003, offline_evaldir='', offline_traindir='', opt='adam', opt_eps=1e-05, oversample_ends=False, precision=16, pred_discount=False, prefill=2500, pretrain=100, reset_every=0, reward_layers=2, reward_scale=1.0, seed=0, size=(64, 64), slow_actor_target=True, slow_target_fraction=1, slow_target_update=100, slow_value_target=True, steps=10000000.0, task='dmc_walker_walk', time_limit=1000, train_every=5, train_steps=1, traindir=None, units=400, value_decay=0.0, value_grad_clip=100, value_head='normal', value_layers=3, value_lr=8e-05, weight_decay=0.0)\n",
    "\n",
    "config.logdir = 'log'\n",
    "config.eval_every = 5e3\n",
    "config.log_every = 5e3\n",
    "config.steps = 1e5 \n",
    "config.device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(config.logdir).expanduser()\n",
    "config.traindir = config.traindir or logdir / 'train_eps'\n",
    "config.evaldir = config.evaldir or logdir / 'eval_eps'\n",
    "config.steps //= config.action_repeat\n",
    "config.eval_every //= config.action_repeat\n",
    "config.log_every //= config.action_repeat\n",
    "config.time_limit //= config.action_repeat\n",
    "config.act = getattr(torch.nn, config.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_steps(folder):\n",
    "  return sum(int(str(n).split('-')[-1][:-4]) - 1 for n in folder.glob('*.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logdir log\n",
      "step  51500\n",
      "after action repeat, step  103000\n"
     ]
    }
   ],
   "source": [
    "print('Logdir', logdir)\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "config.traindir.mkdir(parents=True, exist_ok=True)\n",
    "config.evaldir.mkdir(parents=True, exist_ok=True)\n",
    "step = count_steps(config.traindir)\n",
    "print('step ', step)\n",
    "logger = tools.Logger(logdir, config.action_repeat * step) # 会生成一个log\n",
    "print('after action repeat, step ', logger.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_episode(config, logger, mode, train_eps, eval_eps, episode):\n",
    "  directory = dict(train=config.traindir, eval=config.evaldir)[mode]\n",
    "  cache = dict(train=train_eps, eval=eval_eps)[mode]\n",
    "  filename = tools.save_episodes(directory, [episode])[0]\n",
    "  length = len(episode['reward']) - 1\n",
    "  score = float(episode['reward'].astype(np.float64).sum())\n",
    "  video = episode['image']\n",
    "  if mode == 'eval':\n",
    "    cache.clear()\n",
    "  if mode == 'train' and config.dataset_size:\n",
    "    total = 0\n",
    "    for key, ep in reversed(sorted(cache.items(), key=lambda x: x[0])):\n",
    "      if total <= config.dataset_size - length:\n",
    "        total += len(ep['reward']) - 1\n",
    "      else:\n",
    "        del cache[key]\n",
    "    logger.scalar('dataset_size', total + length)\n",
    "  cache[str(filename)] = episode\n",
    "  print(f'{mode.title()} episode has {length} steps and return {score:.1f}.')\n",
    "  logger.scalar(f'{mode}_return', score)\n",
    "  logger.scalar(f'{mode}_length', length)\n",
    "  logger.scalar(f'{mode}_episodes', len(cache))\n",
    "  if mode == 'eval' or config.expl_gifs:\n",
    "    logger.video(f'{mode}_policy', video[None])\n",
    "  logger.write()\n",
    "\n",
    "def make_env(config, logger, mode, train_eps, eval_eps):\n",
    "  suite, task = config.task.split('_', 1)\n",
    "  if suite == 'dmc':\n",
    "    env = wrappers.DeepMindControl(task, config.action_repeat, config.size)\n",
    "    env = wrappers.NormalizeActions(env)\n",
    "  elif suite == 'atari':\n",
    "    env = wrappers.Atari(\n",
    "        task, config.action_repeat, config.size,\n",
    "        grayscale=config.grayscale,\n",
    "        life_done=False and ('train' in mode),\n",
    "        sticky_actions=True,\n",
    "        all_actions=True)\n",
    "    env = wrappers.OneHotAction(env)\n",
    "  elif suite == 'dmlab':\n",
    "    env = wrappers.DeepMindLabyrinth(\n",
    "        task,\n",
    "        mode if 'train' in mode else 'test',\n",
    "        config.action_repeat)\n",
    "    env = wrappers.OneHotAction(env)\n",
    "  else:\n",
    "    raise NotImplementedError(suite)\n",
    "  env = wrappers.TimeLimit(env, config.time_limit)\n",
    "  env = wrappers.SelectAction(env, key='action')\n",
    "  if (mode == 'train') or (mode == 'eval'):\n",
    "    callbacks = [functools.partial(\n",
    "        process_episode, config, logger, mode, train_eps, eval_eps)]\n",
    "    env = wrappers.CollectDataset(env, callbacks)\n",
    "  env = wrappers.RewardObs(env)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmc_walker_walk\n"
     ]
    }
   ],
   "source": [
    "print(config.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "from dm_control import suite\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['MUJOCO_GL'] = 'glfw'\n",
    "\n",
    "# 加载环境\n",
    "env = suite.load(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "\n",
    "# 重置环境并渲染一帧\n",
    "time_step = env.reset()\n",
    "image = env.physics.render(camera_id=0, width=480, height=360)\n",
    "\n",
    "# 使用matplotlib显示图像\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create envs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xthu/anaconda3/envs/dreamer/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "  print('Create envs.')\n",
    "  if config.offline_traindir:\n",
    "    directory = config.offline_traindir.format(**vars(config))\n",
    "  else:\n",
    "    directory = config.traindir\n",
    "  train_eps = tools.load_episodes(directory, limit=config.dataset_size)\n",
    "  if config.offline_evaldir:\n",
    "    directory = config.offline_evaldir.format(**vars(config))\n",
    "  else:\n",
    "    directory = config.evaldir\n",
    "  eval_eps = tools.load_episodes(directory, limit=1)\n",
    "  make = lambda mode: make_env(config, logger, mode, train_eps, eval_eps)\n",
    "  train_envs = [make('train') for _ in range(config.envs)]\n",
    "  eval_envs = [make('eval') for _ in range(config.envs)]\n",
    "  acts = train_envs[0].action_space\n",
    "  config.num_actions = acts.n if hasattr(acts, 'n') else acts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "print(eval_eps)\n",
    "print(config.num_actions)\n",
    "print(config.offline_traindir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need to call every time\n",
    "if not config.offline_traindir:\n",
    "    prefill = max(0, config.prefill - count_steps(config.traindir))\n",
    "    print(f'Prefill dataset ({prefill} steps).')\n",
    "    if hasattr(acts, 'discrete'):\n",
    "      random_actor = tools.OneHotDist(torch.zeros_like(torch.Tensor(acts.low))[None])\n",
    "    else:\n",
    "      random_actor = torchd.independent.Independent(\n",
    "          torchd.uniform.Uniform(torch.Tensor(acts.low)[None],\n",
    "                                 torch.Tensor(acts.high)[None]), 1)\n",
    "    def random_agent(o, d, s, r):\n",
    "      action = random_actor.sample()\n",
    "      logprob = random_actor.log_prob(action)\n",
    "      return {'action': action, 'logprob': logprob}, None\n",
    "    tools.simulate(random_agent, train_envs, prefill)\n",
    "    tools.simulate(random_agent, eval_envs, episodes=1)\n",
    "    logger.step = config.action_repeat * count_steps(config.traindir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG (in tools.simulate)\n",
    "print(random_actor)\n",
    "base_dist = random_actor.base_dist\n",
    "print(base_dist.low)\n",
    "print(base_dist.high)\n",
    "\n",
    "action = random_actor.sample()\n",
    "print(action)\n",
    "\n",
    "logprob = random_actor.log_prob(action)\n",
    "print(logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103000\n"
     ]
    }
   ],
   "source": [
    "# DEBUG (in tools.simulate)\n",
    "print(logger.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dreamer(nn.Module):\n",
    "\n",
    "  def __init__(self, config, logger, dataset):\n",
    "    super(Dreamer, self).__init__()\n",
    "    self._config = config\n",
    "    self._logger = logger\n",
    "    self._should_log = tools.Every(config.log_every)\n",
    "    self._should_train = tools.Every(config.train_every)\n",
    "    self._should_pretrain = tools.Once()\n",
    "    self._should_reset = tools.Every(config.reset_every)\n",
    "    self._should_expl = tools.Until(int(\n",
    "        config.expl_until / config.action_repeat))\n",
    "    self._metrics = {}\n",
    "    self._step = count_steps(config.traindir)\n",
    "    # Schedules.\n",
    "    config.actor_entropy = (\n",
    "        lambda x=config.actor_entropy: tools.schedule(x, self._step))\n",
    "    config.actor_state_entropy = (\n",
    "        lambda x=config.actor_state_entropy: tools.schedule(x, self._step))\n",
    "    config.imag_gradient_mix = (\n",
    "        lambda x=config.imag_gradient_mix: tools.schedule(x, self._step))\n",
    "    self._dataset = dataset\n",
    "    self._wm = models.WorldModel(self._step, config)\n",
    "    self._task_behavior = models.ImagBehavior(\n",
    "        config, self._wm, config.behavior_stop_grad)\n",
    "    reward = lambda f, s, a: self._wm.heads['reward'](f).mean\n",
    "    self._expl_behavior = dict(\n",
    "        greedy=lambda: self._task_behavior,\n",
    "        random=lambda: expl.Random(config),\n",
    "        plan2explore=lambda: expl.Plan2Explore(config, self._wm, reward),\n",
    "    )[config.expl_behavior]()\n",
    "\n",
    "  def __call__(self, obs, reset, state=None, reward=None, training=True):\n",
    "    step = self._step\n",
    "    if self._should_reset(step):\n",
    "      state = None\n",
    "    if state is not None and reset.any():\n",
    "      mask = 1 - reset\n",
    "      for key in state[0].keys():\n",
    "        for i in range(state[0][key].shape[0]):\n",
    "          state[0][key][i] *= mask[i]\n",
    "      for i in range(len(state[1])):\n",
    "        state[1][i] *= mask[i]\n",
    "    if training and self._should_train(step):\n",
    "      steps = (\n",
    "          self._config.pretrain if self._should_pretrain()\n",
    "          else self._config.train_steps)\n",
    "      for _ in range(steps):\n",
    "        self._train(next(self._dataset))\n",
    "      if self._should_log(step):\n",
    "        for name, values in self._metrics.items():\n",
    "          self._logger.scalar(name, float(np.mean(values)))\n",
    "          self._metrics[name] = []\n",
    "        openl = self._wm.video_pred(next(self._dataset))\n",
    "        self._logger.video('train_openl', to_np(openl))\n",
    "        self._logger.write(fps=True)\n",
    "\n",
    "    policy_output, state = self._policy(obs, state, training)\n",
    "\n",
    "    if training:\n",
    "      self._step += len(reset)\n",
    "      self._logger.step = self._config.action_repeat * self._step\n",
    "    return policy_output, state\n",
    "\n",
    "  def _policy(self, obs, state, training):\n",
    "    if state is None:\n",
    "      batch_size = len(obs['image'])\n",
    "      latent = self._wm.dynamics.initial(len(obs['image']))\n",
    "      action = torch.zeros((batch_size, self._config.num_actions)).to(self._config.device)\n",
    "    else:\n",
    "      latent, action = state\n",
    "    embed = self._wm.encoder(self._wm.preprocess(obs))\n",
    "    latent, _ = self._wm.dynamics.obs_step(\n",
    "        latent, action, embed, self._config.collect_dyn_sample)\n",
    "    if self._config.eval_state_mean:\n",
    "      latent['stoch'] = latent['mean']\n",
    "    feat = self._wm.dynamics.get_feat(latent)\n",
    "    if not training:\n",
    "      actor = self._task_behavior.actor(feat)\n",
    "      action = actor.mode()\n",
    "    elif self._should_expl(self._step):\n",
    "      actor = self._expl_behavior.actor(feat)\n",
    "      action = actor.sample()\n",
    "    else:\n",
    "      actor = self._task_behavior.actor(feat)\n",
    "      action = actor.sample()\n",
    "    logprob = actor.log_prob(action)\n",
    "    latent = {k: v.detach()  for k, v in latent.items()}\n",
    "    action = action.detach()\n",
    "    if self._config.actor_dist == 'onehot_gumble':\n",
    "      action = torch.one_hot(torch.argmax(action, dim=-1), self._config.num_actions)\n",
    "    action = self._exploration(action, training)\n",
    "    policy_output = {'action': action, 'logprob': logprob}\n",
    "    state = (latent, action)\n",
    "    return policy_output, state\n",
    "\n",
    "  def _exploration(self, action, training):\n",
    "    amount = self._config.expl_amount if training else self._config.eval_noise\n",
    "    if amount == 0:\n",
    "      return action\n",
    "    if 'onehot' in self._config.actor_dist:\n",
    "      probs = amount / self._config.num_actions + (1 - amount) * action\n",
    "      return tools.OneHotDist(probs=probs).sample()\n",
    "    else:\n",
    "      return torch.clip(torchd.normal.Normal(action, amount).sample(), -1, 1)\n",
    "    raise NotImplementedError(self._config.action_noise)\n",
    "\n",
    "  def _train(self, data):\n",
    "    metrics = {}\n",
    "    post, context, mets = self._wm._train(data)\n",
    "    metrics.update(mets)\n",
    "    start = post\n",
    "    if self._config.pred_discount:  # Last step could be terminal.\n",
    "      start = {k: v[:, :-1] for k, v in post.items()}\n",
    "      context = {k: v[:, :-1] for k, v in context.items()}\n",
    "    reward = lambda f, s, a: self._wm.heads['reward'](\n",
    "        self._wm.dynamics.get_feat(s)).mode()\n",
    "    metrics.update(self._task_behavior._train(start, reward)[-1])\n",
    "    if self._config.expl_behavior != 'greedy':\n",
    "      if self._config.pred_discount:\n",
    "        data = {k: v[:, :-1] for k, v in data.items()}\n",
    "      mets = self._expl_behavior.train(start, context, data)[-1]\n",
    "      metrics.update({'expl_' + key: value for key, value in mets.items()})\n",
    "    for name, value in metrics.items():\n",
    "      if not name in self._metrics.keys():\n",
    "        self._metrics[name] = [value]\n",
    "      else:\n",
    "        self._metrics[name].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulate agent.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m agent\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (logdir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 13\u001b[0m     agent\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatest_model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     agent\u001b[38;5;241m.\u001b[39m_should_pretrain\u001b[38;5;241m.\u001b[39m_once \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:592\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    591\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 592\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:851\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mUnpickler(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    850\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 851\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:843\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 843\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:832\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    829\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    831\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 832\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/_utils.py:80\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/cuda/__init__.py:484\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m _lazy_init()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "def make_dataset(episodes, config):\n",
    "  generator = tools.sample_episodes(\n",
    "      episodes, config.batch_length, config.oversample_ends)\n",
    "  dataset = tools.from_generator(generator, config.batch_size)\n",
    "  return dataset\n",
    "\n",
    "print('Simulate agent.')\n",
    "train_dataset = make_dataset(train_eps, config)\n",
    "eval_dataset = make_dataset(eval_eps, config)\n",
    "agent = Dreamer(config, logger, train_dataset).to(config.device)\n",
    "agent.requires_grad_(requires_grad=False)\n",
    "if (logdir / 'latest_model.pt').exists():\n",
    "    agent.load_state_dict(torch.load(logdir / 'latest_model.pt'))\n",
    "    agent._should_pretrain._once = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(episodes, config):\n",
    "  generator = tools.sample_episodes(\n",
    "      episodes, config.batch_length, config.oversample_ends)\n",
    "  dataset = tools.from_generator(generator, config.batch_size)\n",
    "  return dataset\n",
    "\n",
    "print('Simulate agent.')\n",
    "train_dataset = make_dataset(train_eps, config)\n",
    "eval_dataset = make_dataset(eval_eps, config)\n",
    "agent = Dreamer(config, logger, train_dataset).to(config.device)\n",
    "agent.requires_grad_(requires_grad=False)\n",
    "if (logdir / 'latest_model.pt').exists():\n",
    "    agent.load_state_dict(torch.load(logdir / 'latest_model.pt'))\n",
    "    agent._should_pretrain._once = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latest_step(step):\n",
    "    with open(os.path.join(config.logdir, \"latest_step.txt\"),'w') as f:\n",
    "        f.write(str(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原本的\n",
    "state = None\n",
    "best_eval_return = 0\n",
    "while agent._step < config.steps:\n",
    "    logger.write()\n",
    "    print('Start evaluation.')\n",
    "    video_pred = agent._wm.video_pred(next(eval_dataset))\n",
    "    logger.video('eval_openl', to_np(video_pred))\n",
    "    eval_policy = functools.partial(agent, training=False)\n",
    "    eval_state = tools.simulate(eval_policy, eval_envs, episodes=1)\n",
    "    eval_return = eval_state[7]\n",
    "    print('Start training.')\n",
    "    state = tools.simulate(agent, train_envs, config.eval_every, state=state)\n",
    "    torch.save(agent.state_dict(), logdir / 'latest_model.pt')\n",
    "    if eval_return > best_eval_return:\n",
    "        torch.save(agent.state_dict(), logdir / f'best_model_step-{agent._step}.pt')\n",
    "        print(f\"The best model saved at {agent._step}\")\n",
    "        best_eval_return = eval_return\n",
    "    save_latest_step(agent._step)\n",
    "    print(f\"The latest model saved at {agent._step}\")\n",
    "for env in train_envs + eval_envs:\n",
    "    try:\n",
    "      env.close()\n",
    "    except Exception:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
