{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56d4e83-455e-43ff-842c-c59099b9781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 13 05:37:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090         Off| 00000000:01:00.0 Off |                  Off |\n",
      "| 45%   63C    P2              301W / 450W|  24091MiB / 24564MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090         Off| 00000000:2D:00.0 Off |                  Off |\n",
      "| 34%   55C    P2              159W / 450W|  21233MiB / 24564MiB |     84%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090         Off| 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   48C    P2              291W / 450W|  20363MiB / 24564MiB |     72%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090         Off| 00000000:61:00.0 Off |                  Off |\n",
      "| 30%   34C    P8               31W / 450W|     11MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    780758      C   python                                    24078MiB |\n",
      "|    0   N/A  N/A   1153502      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A   1157015      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A   1128418      C   python                                    21220MiB |\n",
      "|    1   N/A  N/A   1153502      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A   1157015      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    2   N/A  N/A   1153502      G   /usr/lib/xorg/Xorg                           35MiB |\n",
      "|    2   N/A  N/A   1157015      G   /usr/lib/xorg/Xorg                           62MiB |\n",
      "|    2   N/A  N/A   1157586      G   /usr/bin/gnome-shell                         60MiB |\n",
      "|    2   N/A  N/A   1862572      C   python                                    20178MiB |\n",
      "|    3   N/A  N/A   1153502      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    3   N/A  N/A   1157015      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526b4194-b9c3-4af0-8428-959cc9e1b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used:  cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device_str = 'cuda:3'\n",
    "print('device used: ', device_str)\n",
    "device = torch.device(device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20524e9-57fd-43cc-8c11-580671b7bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_status():\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    reserved_memory = torch.cuda.memory_reserved(device)\n",
    "    free_memory = total_memory - allocated_memory\n",
    "    \n",
    "    print(f\"Total memory: {total_memory / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Allocated memory: {allocated_memory / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Reserved memory: {reserved_memory / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Free memory: {free_memory / (1024 ** 2):.2f} MB\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85aa5b1e-9919-414f-b1ab-27dc834038f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 24217.31 MB\n",
      "Allocated memory: 0.00 MB\n",
      "Reserved memory: 1910.00 MB\n",
      "Free memory: 24217.31 MB\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8758fc5d-ab7e-4af8-8331-614c4b0f9d1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/cuda/memory.py:114\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()  # 确保 CUDA 操作完成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe928f2-2340-42a2-9293-192087f7ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory status:\n",
      "Total memory: 24217.31 MB\n",
      "Allocated memory: 0.00 MB\n",
      "Reserved memory: 0.00 MB\n",
      "Free memory: 24217.31 MB\n",
      "----------------------------------------\n",
      "Allocating large tensors to CUDA device...\n",
      "Memory status after allocation:\n",
      "Total memory: 24217.31 MB\n",
      "Allocated memory: 1910.00 MB\n",
      "Reserved memory: 1910.00 MB\n",
      "Free memory: 22307.31 MB\n",
      "----------------------------------------\n",
      "Releasing tensors from CUDA device...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReleasing tensors from CUDA device...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensors\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory status after releasing tensors:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m print_memory_status()\n",
      "File \u001b[0;32m~/anaconda3/envs/dreamer/lib/python3.8/site-packages/torch/cuda/memory.py:114\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "print(\"Initial memory status:\")\n",
    "print_memory_status()\n",
    "\n",
    "tensor_size = (10000, 10000)\n",
    "print(\"Allocating large tensors to CUDA device...\")\n",
    "tensors = [torch.randn(tensor_size, device=device) for _ in range(5)]\n",
    "print(\"Memory status after allocation:\")\n",
    "print_memory_status()\n",
    "\n",
    "print(\"Releasing tensors from CUDA device...\")\n",
    "del tensors\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory status after releasing tensors:\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafdd0a7-42ef-4ec8-9368-e75bce61c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory status after another allocation:\n",
      "Total memory: 24209.12 MB\n",
      "Allocated memory: 382.00 MB\n",
      "Reserved memory: 1910.00 MB\n",
      "Free memory: 23827.12 MB\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "another_tensor = torch.randn(tensor_size, device=device)\n",
    "print(\"Memory status after another allocation:\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620f85f-3d41-48f6-a2ea-72748bd95b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ec0d7-89e7-41dd-b290-419ecb5adde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = argparse.Namespace(act='ELU', action_repeat=2, actor_disc=5, actor_dist='trunc_normal', actor_entropy='1e-4', actor_grad_clip=100, actor_init_std=1.0, actor_layers=4, actor_lr=8e-05, actor_min_std=0.1, actor_outscale=0.0, actor_state_entropy=0.0, actor_temp=0.1, batch_length=50, batch_size=50, behavior_stop_grad=True, clip_rewards='identity', cnn_depth=32, collect_dyn_sample=True, dataset_size=0, debug=False, decoder_kernels=(5, 5, 6, 6), decoder_thin=True, device='cuda:0', disag_action_cond=False, disag_layers=4, disag_log=True, disag_models=10, disag_offset=1, disag_target='stoch', disag_units=400, discount=0.99, discount_lambda=0.95, discount_layers=3, discount_scale=1.0, dyn_cell='gru', dyn_deter=200, dyn_discrete=0, dyn_hidden=200, dyn_input_layers=1, dyn_mean_act='none', dyn_min_std=0.1, dyn_output_layers=1, dyn_rec_depth=1, dyn_shared=False, dyn_std_act='sigmoid2', dyn_stoch=50, dyn_temp_post=True, encoder_kernels=(4, 4, 4, 4), envs=1, eval_every=10000.0, eval_noise=0.0, eval_state_mean=False, evaldir=None, expl_amount=0.0, expl_behavior='greedy', expl_extr_scale=0.0, expl_gifs=False, expl_intr_scale=1.0, expl_until=0, future_entropy=False, grad_clip=100, grad_heads=('image', 'reward'), grayscale=False, imag_gradient='dynamics', imag_gradient_mix='0.1', imag_horizon=15, imag_sample=True, kl_balance='0.8', kl_forward=False, kl_free='1.0', kl_scale='1.0', log_every=10000.0, logdir=None, model_lr=0.0003, offline_evaldir='', offline_traindir='', opt='adam', opt_eps=1e-05, oversample_ends=False, precision=16, pred_discount=False, prefill=2500, pretrain=100, reset_every=0, reward_layers=2, reward_scale=1.0, seed=0, size=(64, 64), slow_actor_target=True, slow_target_fraction=1, slow_target_update=100, slow_value_target=True, steps=10000000.0, task='dmc_walker_walk', time_limit=1000, train_every=5, train_steps=1, traindir=None, units=400, value_decay=0.0, value_grad_clip=100, value_head='normal', value_layers=3, value_lr=8e-05, weight_decay=0.0)\n",
    "\n",
    "config.logdir = '摄影师'\n",
    "config.eval_every = 5e3\n",
    "config.log_every = 5e3\n",
    "config.steps = 1e5 \n",
    "state_dict_path = 'log/latest_model.pt'\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0428a23-7876-47fa-90c3-b58658464718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dreamer(nn.Module):\n",
    "\n",
    "  def __init__(self, config, logger, dataset):\n",
    "    super(Dreamer, self).__init__()\n",
    "    self._config = config\n",
    "    self._logger = logger\n",
    "    self._should_log = tools.Every(config.log_every)\n",
    "    self._should_train = tools.Every(config.train_every)\n",
    "    self._should_pretrain = tools.Once()\n",
    "    self._should_reset = tools.Every(config.reset_every)\n",
    "    self._should_expl = tools.Until(int(\n",
    "        config.expl_until / config.action_repeat))\n",
    "    self._metrics = {}\n",
    "    self._step = count_steps(config.traindir)\n",
    "    # Schedules.\n",
    "    config.actor_entropy = (\n",
    "        lambda x=config.actor_entropy: tools.schedule(x, self._step))\n",
    "    config.actor_state_entropy = (\n",
    "        lambda x=config.actor_state_entropy: tools.schedule(x, self._step))\n",
    "    config.imag_gradient_mix = (\n",
    "        lambda x=config.imag_gradient_mix: tools.schedule(x, self._step))\n",
    "    self._dataset = dataset\n",
    "    self._wm = models.WorldModel(self._step, config)\n",
    "    self._task_behavior = models.ImagBehavior(\n",
    "        config, self._wm, config.behavior_stop_grad)\n",
    "    reward = lambda f, s, a: self._wm.heads['reward'](f).mean\n",
    "    self._expl_behavior = dict(\n",
    "        greedy=lambda: self._task_behavior,\n",
    "        random=lambda: expl.Random(config),\n",
    "        plan2explore=lambda: expl.Plan2Explore(config, self._wm, reward),\n",
    "    )[config.expl_behavior]()\n",
    "\n",
    "  def __call__(self, obs, reset, state=None, reward=None, training=True):\n",
    "    step = self._step\n",
    "    if self._should_reset(step):\n",
    "      state = None\n",
    "    if state is not None and reset.any():\n",
    "      mask = 1 - reset\n",
    "      for key in state[0].keys():\n",
    "        for i in range(state[0][key].shape[0]):\n",
    "          state[0][key][i] *= mask[i]\n",
    "      for i in range(len(state[1])):\n",
    "        state[1][i] *= mask[i]\n",
    "    if training and self._should_train(step):\n",
    "      steps = (\n",
    "          self._config.pretrain if self._should_pretrain()\n",
    "          else self._config.train_steps)\n",
    "      for _ in range(steps):\n",
    "        self._train(next(self._dataset))\n",
    "      if self._should_log(step):\n",
    "        for name, values in self._metrics.items():\n",
    "          self._logger.scalar(name, float(np.mean(values)))\n",
    "          self._metrics[name] = []\n",
    "        openl = self._wm.video_pred(next(self._dataset))\n",
    "        self._logger.video('train_openl', to_np(openl))\n",
    "        self._logger.write(fps=True)\n",
    "\n",
    "    policy_output, state = self._policy(obs, state, training)\n",
    "\n",
    "    if training:\n",
    "      self._step += len(reset)\n",
    "      self._logger.step = self._config.action_repeat * self._step\n",
    "    return policy_output, state\n",
    "\n",
    "  def _policy(self, obs, state, training):\n",
    "    if state is None:\n",
    "      batch_size = len(obs['image'])\n",
    "      latent = self._wm.dynamics.initial(len(obs['image']))\n",
    "      action = torch.zeros((batch_size, self._config.num_actions)).to(self._config.device)\n",
    "    else:\n",
    "      latent, action = state\n",
    "    embed = self._wm.encoder(self._wm.preprocess(obs))\n",
    "    latent, _ = self._wm.dynamics.obs_step(\n",
    "        latent, action, embed, self._config.collect_dyn_sample)\n",
    "    if self._config.eval_state_mean:\n",
    "      latent['stoch'] = latent['mean']\n",
    "    feat = self._wm.dynamics.get_feat(latent)\n",
    "    if not training:\n",
    "      actor = self._task_behavior.actor(feat)\n",
    "      action = actor.mode()\n",
    "    elif self._should_expl(self._step):\n",
    "      actor = self._expl_behavior.actor(feat)\n",
    "      action = actor.sample()\n",
    "    else:\n",
    "      actor = self._task_behavior.actor(feat)\n",
    "      action = actor.sample()\n",
    "    logprob = actor.log_prob(action)\n",
    "    latent = {k: v.detach()  for k, v in latent.items()}\n",
    "    action = action.detach()\n",
    "    if self._config.actor_dist == 'onehot_gumble':\n",
    "      action = torch.one_hot(torch.argmax(action, dim=-1), self._config.num_actions)\n",
    "    action = self._exploration(action, training)\n",
    "    policy_output = {'action': action, 'logprob': logprob}\n",
    "    state = (latent, action)\n",
    "    return policy_output, state\n",
    "\n",
    "  def _exploration(self, action, training):\n",
    "    amount = self._config.expl_amount if training else self._config.eval_noise\n",
    "    if amount == 0:\n",
    "      return action\n",
    "    if 'onehot' in self._config.actor_dist:\n",
    "      probs = amount / self._config.num_actions + (1 - amount) * action\n",
    "      return tools.OneHotDist(probs=probs).sample()\n",
    "    else:\n",
    "      return torch.clip(torchd.normal.Normal(action, amount).sample(), -1, 1)\n",
    "    raise NotImplementedError(self._config.action_noise)\n",
    "\n",
    "  def _train(self, data):\n",
    "    metrics = {}\n",
    "    post, context, mets = self._wm._train(data)\n",
    "    metrics.update(mets)\n",
    "    start = post\n",
    "    if self._config.pred_discount:  # Last step could be terminal.\n",
    "      start = {k: v[:, :-1] for k, v in post.items()}\n",
    "      context = {k: v[:, :-1] for k, v in context.items()}\n",
    "    reward = lambda f, s, a: self._wm.heads['reward'](\n",
    "        self._wm.dynamics.get_feat(s)).mode()\n",
    "    metrics.update(self._task_behavior._train(start, reward)[-1])\n",
    "    if self._config.expl_behavior != 'greedy':\n",
    "      if self._config.pred_discount:\n",
    "        data = {k: v[:, :-1] for k, v in data.items()}\n",
    "      mets = self._expl_behavior.train(start, context, data)[-1]\n",
    "      metrics.update({'expl_' + key: value for key, value in mets.items()})\n",
    "    for name, value in metrics.items():\n",
    "      if not name in self._metrics.keys():\n",
    "        self._metrics[name] = [value]\n",
    "      else:\n",
    "        self._metrics[name].append(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
